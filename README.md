# ğŸ§  Continual Learning with Transformers (EWC + Replay)

This project demonstrates how to perform continual learning on a sequence of NLP classification tasks using RoBERTa. It implements and compares three strategies:

- âœ… Baseline (no mitigation for forgetting)
- âœ… EWC (Elastic Weight Consolidation)
- âœ… Replay Buffer

It is built with Hugging Face Transformers and Datasets, and designed to be extensible, reproducible, and insightful for research and hiring demonstrations.

---

## ğŸ” What is Continual Learning?

Continual learning is the ability to train a model on new tasks without forgetting previous ones. Standard neural networks suffer from **catastrophic forgetting**, where performance on older tasks drops as new tasks are learned.

This repo tackles that with two popular strategies:

| Technique | Description |
|----------|-------------|
| EWC      | Penalizes changes to important weights for previous tasks |
| Replay   | Reuses a buffer of old examples when training on new tasks |

---

## ğŸ—‚ï¸ Task Setup

We train the model on a sequence of text classification tasks:

1. ğŸ­ IMDb â†’ Sentiment classification (positive/negative)
2. ğŸ“° AG News â†’ Topic classification (e.g., World, Sports, Business)
3. ğŸ¯ SNIPS â†’ Intent classification (e.g., play music, get weather)

These tasks represent diverse real-world NLP challenges.

---

## ğŸ“ Directory Structure

```
continual-learning-transformers/
â”œâ”€â”€ data/                  # CSVs for IMDb, AG News, SNIPS (replaceable with full datasets)
â”œâ”€â”€ model/                 # (Reserved for future use)
â”œâ”€â”€ plots/                 # Output evaluation plots (accuracy over time)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_loader.py     # Loads tasks from CSV or Hugging Face datasets
â”‚   â”œâ”€â”€ ewc.py             # EWC implementation
â”‚   â””â”€â”€ replay_buffer.py   # Replay buffer implementation
â”œâ”€â”€ train_baseline.py      # No forgetting mitigation
â”œâ”€â”€ train_ewc.py           # With EWC penalty
â”œâ”€â”€ train_replay.py        # With replay buffer
â”œâ”€â”€ requirements.txt       # Required Python libraries
â”œâ”€â”€ config.json            # Training hyperparameters
â””â”€â”€ README.md              # This file
```

---

## âš™ï¸ How to Run

Install dependencies:
```bash
pip install -r requirements.txt
```

Run any of the three learning approaches:

```bash
python train_baseline.py
python train_ewc.py
python train_replay.py
```

---

## ğŸ“Š Results and Evaluation

Each script prints task-wise accuracy. You can easily add evaluation plots in the `plots/` folder (coming soon).

---

## ğŸ“š Data Notes

You can either:
- Use the small CSV files in /data/ for quick tests
- Or modify `utils/data_loader.py` to load full datasets from Hugging Face like so:

```python
from datasets import load_dataset
ag_news = load_dataset("ag_news", split="train")
```

---

## ğŸ§  Why This Project Matters

| Signal | Value |
|--------|-------|
| ğŸ”„ Continual Learning | Shows your awareness of lifelong learning research |
| ğŸ¯ Task Diversity | Covers classification, topic detection, and user intent |
| ğŸ“ˆ Research Skills | Demonstrates regularization, memory replay, and benchmarks |
| ğŸ§ª Extension Ready | You can plug in other methods like LwF, GEM, or adapters |

---

## ğŸ¤ Credits

This project was generated with guidance from ChatGPT and custom code to reflect current AI research directions in continual learning.

---

## ğŸ“œ License

MIT License. Use freely with attribution.
